# 2022-04-04
# Machine Learning
## Stocastic gradient decents
Usa un mini batch $p<<m$
Ogni epoca controlliamo $\lfloor\frac{m}{p}\rfloor$ mini batch

---

Variazione del learning rate $\alpha$:

- Time-based: $\alpha_{t}=\alpha_{0}\cdot \frac{1}{1+kt}$
- Step-decay: $\alpha_{t+1}=\alpha_{t}\cdot \frac{1}{2^{e}}$

!def Adagard
Aggiornamenti con learn rating più grandi per i laori sparsi e piccoli, per quelli non sparsi
def!

!def Adam
Utilizza un learning rate per ognuno dei parametri
def!

---

IL momentum è un parametro $\beta$ che identifica il peso del risultato $z$ nella formula della discesa del gradiente
$z^{\text{new}}\leftarrow \beta z^{\text{old}} + \frac{\delta J(\theta)}{\delta\theta}$

---

Alcune metriche utili

- Matrice di confusione
- Accuracy
- F1
  - Precision
  - Recall
- ROC curve

Matrice di confusione

|   | 1 | 0 |
| - | - | - |
| 1 | a | c |
| 0 | d | b |
- TP(Correttamente Positivo): $\frac{a}{a+c}$
- TN(Correttamente Negativ): $\frac{b}{b+d}$
- FP: $\frac{d}{b+d}$
- FN: $\frac{c}{a+c}$
