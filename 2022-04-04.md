# 2022-04-04
# Machine Learning
## Stocastic gradient decents
Usa un mini batch $p<<m$
Ogni epoca controlliamo $\lfloor\frac{m}{p}\rfloor$ mini batch

---

Variazione del learning rate $\alpha$:

- Time-based: $\alpha_{t}=\alpha_{0}\cdot \frac{1}{1+kt}$
- Step-decay: $\alpha_{t+1}=\alpha_{t}\cdot \frac{1}{2^{e}}$

!def Adagard
Aggiornamenti con learn rating più grandi per i laori sparsi e piccoli, per quelli non sparsi
def!

!def Adam
Utilizza un learning rate per ognuno dei parametri
def!

---

IL momentum è un parametro $\beta$ che identifica il peso del risultato $z$ nella formula della discesa del gradiente
$z^{\text{new}}\leftarrow \beta z^{\text{old}} + \frac{\delta J(\theta)}{\delta\theta}$

---

Alcune metriche utili

- Matrice di confusione
- Accuracy
- F1
  - Precision
  - Recall
- ROC curve

Matrice di confusione

> Fare attenzione alle dimensioni (Reale, Predizione).
> In questo caso sono le righe *Reali* e le colonne *Predizioni*

> La proporzione dei reali è importante

|   | 1 | 0 |
| - | - | - |
| 1 | a | c |
| 0 | d | b |

- TP(Correttamente Positivo): $\frac{a}{a+c}$
- TN(Correttamente Negativo): $\frac{b}{b+d}$
- FP(Scorrettamente Positivo): $\frac{d}{b+d}$
- FN(Scorrettamente Negativo): $\frac{c}{a+c}$
- Accuracy: $\frac{a+b}{a+b+c+d}$
- Precision: $\frac{a}{a+d}$
- Recall (=TP): $\frac{a}{a+c}$
- ROC Curve: $(\vec{\text{Recall}},O,\vec{\text{Precision}})$
  In base ad una *threasold* si può dare importanza alla *Recall* o alla *Precision*
- F1: $2\frac{\text{Precision}\cdot\text{Recall}}{\text{Precision}+\text{Recall}}$

---

Entropia

> Entropia della stessa distribuzione di probabilità
> $H(X)=-\sum\limits_{k=0}^{K-1}P_{k}(X)log(P_{k}(X))$

Viene usata per dire se siamo sicuri o meno del risultato analizzando l'incertezza delle probabilità di essere di una determinata classe

> Entropia di diverse distribuzioni di probabilità
> $H(X)=-\sum\limits_{k=0}^{K-1}P^{(\text{ideale})}_{k}(X)log(P^{(\text{predetto})}_{k}(X))$

Viene usata per quantizzare la similitudine(o dissimilitudine per valori di entropia più elevati) di due distribuzioni di probabilità

---

Softmax
$x^{(1)}=\mathbb{R}^{d}$
$y^{(i)}\in\{0,1,2,\ldots,k-1\}$

one hot encoding
$\overset{k\times1}{y^{(i)}}=\begin{bmatrix}0\\0\\1\\0\\0\\0\\\vdots\\0\end{bmatrix}$ dove $\sum\limits_{j=0}^{k-1}y^{(i)}_{j}=1$

$h_{\theta}(X)=\begin{bmatrix}P_{\theta^{(0)}}(y=0|X) \\ P_{\theta^{(1)}}(y=1|X) \\ \vdots \\ P_{\theta^{(k-1)}}(y=k-1|X)\end{bmatrix}$

---

$P(y=c|X)=\frac{e^{\theta^{(c)T}x}}{\sum\limits_{k=0}^{k-1}e^{\theta^{(k)T}x}}$
$J(\theta)=-\frac{1}{m}[\sum\limits_{i=1}^{m}(\sum\limits_{c=1}^{k-1}1\{y^{(i)}=c\}\log(h^{(c)}_{\theta}(x^{(o)})))]$
$1\{P\}=\begin{cases}1&\text{ se positivo}\\0&\text{ se negativo}\end{cases}$

$+\frac{\lambda}{2m}\sum\limits_{c=0}^{k-1}\sum\limits_{j=1}^{d}(\theta^{(c)}_{j})^2$

$\frac{e^{(\theta^{(c)}-\psi)^{T}x^{(i)}}}{\sum\limits_{k=0}^{k-1}e^{(\theta^{(k)}-\psi)^{T}x^{(i)}}}$
