# 2022-03-23
![[Recording 20220323124833.webm]]
# Internet Security
## Cose da cercare
- Least Privilege
## Lezione
!def Disponibilità
Il sistema sia operante e funzionante in ogni momento
!eg
Disponibilità di un sito, di un servizio...
eg!
def!

!def Controllo d'accesso
ciascun utente abbia accesso a tutte e sole le risorse o i servi per i quali è autorizzato
!eg
Accesso fisico ad aree di un edificio, accesso digitale a spazio disco...
eg!
def!

!def profili autorizzativi
definizione delle autorizzazione di una determinata entità seguendo la policy
def!
# Machine Learning
$\hat\theta=\min\limits_{\theta}J(\theta)=(T^TT)^-1T^TY$

$$
T=\begin{bmatrix}
x_0^{1} &\cdots &x_n^{(1)}\\
\vdots  &\ddots &\vdots\\
x_0^{m} &\cdots &x_n^{(m)}
\end{bmatrix}
$$

$$
Y=\begin{bmatrix}
y^{1}\\
\vdots\\
y^{m}
\end{bmatrix}\in\mathbb{R}^m
$$

$T^T=U\Sigma V^T$
dove

- $\overset{m\times m}{U}$
- $\overset{m\times n}{\Sigma}$
- $\overset{n\times n}{V^T}$

$T^+=V\Sigma^+U^T$

$$
\Sigma=\begin{bmatrix}
\sigma_1 &&& 0 \\
& \ddots && \\
&& \sigma_r & \\
0&&& \ddots
\end{bmatrix}
$$

$\sigma_1>\sigma_2>\ldots>\sigma_r$
$\Sigma^+$ sono i reciproci dei valori $\sigma_i$ in $\Sigma$

| Gradient descent                        | Equazione neurale                                     |
| --------------------------------------- | ----------------------------------------------------- |
| scegliere un L                          | X                                                     |
| # iterazioni                            | X                                                     |
| funziona bene anche quando $n$ è grande | $(T^TT)\simeq\Theta(n^3)$ oneroso quando $n$ è grande |

!def overfitting
- modello molto complesso
	- errore basso sul training set
	- errore alto sul validation/test set
- Pochi campioni nella popolazione per la fase di training
def!

Soluzione all'overfittin: Regolarizzazione (della funzione costo)

$||\theta||=(\sum\limits_{j=1}^{n}{|\theta_j|^p})^{\frac{1}{p}}$
$p=1$: $\sum\limits_{j=1}^{n}{|\theta_j|}$
$p=2$: $\sqrt{\sum\limits_{j=1}^{n}{\theta_j^2}}$
$J(\theta)=\text{<Fitting>}+\text{<Previene Overfitting>} = \frac{1}{2m}(\sum(h_\theta(x)-y)^2 + \lambda\sum\limits_{j=1}^{n}\theta_j^2)$
$\text{<fitting>}$: $\frac{1}{2m}(\sum(h_\theta(x)-y)^2$
$\text{<anti-overfitting>}$: $\lambda\sum\limits_{j=1}^{n}\theta_j^2)$
$\lambda$: ***HyperParameter*** che modula il peso dell'**anti-overfitting**, valori troppo alti portono all'*underfitting* e valori troppo piccoli portano all'*overfitting*

$\theta_j\leftarrow\theta_j-\alpha\frac{\delta J(\theta)}{\delta\theta_j}$
$\frac{\delta J(\theta)}{\delta\theta_j}=\frac{1}{m}\sum\limits_{j=1}^{m}(h_\theta(x^{(1)})-y^{(1)})x_j + \frac{\lambda}{m}\theta_j\;\;\;\;\forall j=1\ldots n$
Note:
Fare attenzione a non usare la funzione costo regolarizzata quando si verifica l'errore con il ***test set***

Possibili funzioni d'errore:
- ***MSE***(Mean Square Error): $\lambda\sum(\theta)^2$
- ***MAE***(Mean Absolute Error): $\lambda\sum|\theta|$
- ***RMSE***(Root Mean Square Error): $\sqrt{\lambda\sum(\theta)^2}$

$Accuracy(e) = \frac{|(x,y):\;loss(h_\theta(x^{(i)}),y^{(i)})\leq e\;\forall i|}{m}$

Funzione regolarizzata ma errori grandi nel **test set**
1. Aumentare i campioni di training
2. provare a ridurre # features
3. nuove features
4. modelli più complessi
5. tuning di $\lambda$

Test diagnostici che individuano il problema:
- Valutazione di modelli
  $D\begin{cases}\text{Training} \\\text{Validation} \\\text{Test}\\\end{cases}$
  Randomico

$k$-fold
$\bbox[border:1px solid white]{T}\bbox[border:1px solid white]{T}\bbox[border:1px solid white]{T}\bbox[border:1px solid white]{T}\bbox[border:1px solid white]{V}\rightarrow\theta^{(i)}$
$\bbox[border:1px solid white]{V}\bbox[border:1px solid white]{T}\bbox[border:1px solid white]{T}\bbox[border:1px solid white]{T}\bbox[border:1px solid white]{T}\rightarrow\theta^{(i)}$
$\bbox[border:1px solid white]{V}\bbox[border:1px solid white]{T}\bbox[border:1px solid white]{T}\bbox[border:1px solid white]{T}\bbox[border:1px solid white]{T}\rightarrow\theta^{(i)}$
\vdots
$\bbox[border:1px solid white]{T}\bbox[border:1px solid white]{T}\bbox[border:1px solid white]{T}\bbox[border:1px solid white]{T}\bbox[border:1px solid white]{V}\rightarrow\theta^{(5)}$