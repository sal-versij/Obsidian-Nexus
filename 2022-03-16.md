# 2022-03-16
# Machine Learning
(Dobbiamo pensare al progetto... subito...)
## TOC
- Recap
	- Sistemi di raccomandazione
	- Polinomial Regression
	- Valutazione della regressione
	- Overfitting

## Cosa serve per creare il codice di un perceptron 
Modello: $h_\theta=\theta_0 x_0+\theta_1 x_1+\ldots+\theta_2 x_2 \implies \theta^Tx$
Funzione costo: $C(\theta)=\frac{1}{2m}\sum\limits_i(h_\theta(x^{(i)})-y^{(i)})^2$
Algoritmo di learning: Discesa del gradiente
$\theta_j\leftarrow\theta_j-\alpha(\frac{\delta C(\theta)}{\delta\theta_j})\forall j$ 
> L'algoritmo di learning ripete $k$ volte $\theta_j\leftarrow\theta_j-\alpha(\frac{\delta C(\theta)}{\delta\theta_j})\forall j$ 

Convergenza: $e^{-\text{qualcosa che non leggo}}$
$\frac{\delta C(\theta)}{\delta\theta_j}=\frac{1}{m}\sum\limits_i(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j$

Siamo in un dominio normalizzato:
$x^{(i)}_j\in[0,1]$

(spiega cose strane sulle dimensioni di case ed il loro numero per spiegare... boh credo le dimensionalità o la normalizzazione... si normalizzazione sembra)
Per riuscire a far convergere l'algoritmo della ***Discesa del gradiente*** efficentemente bisogna rimappare gli input in modo adeguato
(ha fatto vedere un'esempio di come la discesa del gradiente di input rimappati creano un bolla circolare invece che ovale e diventa più veloce)