# 2022-03-16
# Machine Learning
(Dobbiamo pensare al progetto... subito...)
## TOC
- Recap
  - Sistemi di raccomandazione
  - Polinomial Regression
  - Valutazione della regressione
  - Overfitting
## Cosa serve per creare il codice di un perceptron
Modello: $h_\theta=\theta_0 x_0+\theta_1 x_1+\ldots+\theta_2 x_2 \implies \theta^Tx$
Funzione costo: $C(\theta)=\frac{1}{2m}\sum\limits_i(h_\theta(x^{(i)})-y^{(i)})^2$
Algoritmo di learning: ***Discesa del Gradiente*** (Di tipo *batch*) (anche chiamato ***Stocastic Gradient Descend***)
$\theta_j\leftarrow\theta_j-\alpha(\frac{\delta C(\theta)}{\delta\theta_j})\forall j$
con $\alpha>0$

> L'algoritmo di learning ripete $k$ volte $\theta_j\leftarrow\theta_j-\alpha(\frac{\delta C(\theta)}{\delta\theta_j})\forall j$

> In base alla $\alpha$ che scegliamo il grafico che descrive l'andamento della ***Discesa del Gradiente*** può prendere varie forme, magari inizia a scendere ma poi risale perchè $\alpha$ troppo elevato, invece se è molto lento a scendere $\alpha$ potrebbe essere troppo basso

> Con questo algoritmo e questa funzione di costo è possibile analizzare anche funzioni più complesse di $h_\theta$ e potrebbe avere vari minimi locali, in genere un minimo locale è buono tanto quanto gli altri(ci possono essere eccezioni)

Convergenza: $e^{-\text{qualcosa che non leggo}}$
Calcolo della derivata della funzione costo rispoetto a $\theta_j$:
$\frac{\delta C(\theta)}{\delta\theta_j}=\frac{1}{m}\sum\limits_i(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j$

Siamo in un dominio normalizzato:
$x^{(i)}_j\in[0,1]$

(spiega cose strane sulle dimensioni di case ed il loro numero per spiegare... boh credo le dimensionalità o la normalizzazione... si normalizzazione sembra)
Per riuscire a far convergere l'algoritmo della ***Discesa del Gradiente*** efficentemente bisogna rimappare gli input in modo adeguato
(ha fatto vedere un'esempio di come la discesa del gradiente di input rimappati creano un bolla circolare invece che ovale e diventa più veloce)

!def Algoritmo di learning *batch*
Rende possibile eseguire l'algoritmo su di un campionamento $m_s$ sotto insieme del training set
def!

!eg t-esimo istante della discesa del gradiente
$\theta_{0,t} = 1,\theta_{1,t} = 2$
$h_\theta(x^{(i)})=\theta_0+\theta_1x_1$
calcolare i valori di $\theta_0$ e $\theta_1$ all'istante t+2
Training Set

| x_0 | x_1 | y |
| --- | --- | - |
| 1   | 1   | 0 |
| 1   | 0   | 2 |

$\theta_{0,t+1}\leftarrow\theta_{0,t}-\frac{1}{2}\sum\limits_{i=1}^{2}(\theta_{0,t}+\theta_{1,t}x^{(i)}_1-y^{(i)})x_0$
$\theta_{1,t+1}\leftarrow\theta_{1,t}-\frac{1}{2}\sum\limits_{i=1}^{2}(\theta_{0,t}+\theta_{1,t}x^{(i)}_1-y^{(i)})x_1$
Ripetere pr ottenere $\theta_{0,t+2}$ e $\theta_{1,t+2}$

(Esercizio: completarlo a casa... non lo farò mai ma sh)
eg!

(Un tizio dietro di me dice una cosa interessante)
Se consideriamo la derivata della funzione di costo possiamo renderci conto che sta risalendo(la funzione) perchè cambia segno(la derivata), ma non è facilmente applicabile e forse ti da più problemi che altro...

!eg
Assegnare il giusto valore di $\alpha$ al giusto grafico (ora come cazzo faccio i grafici? ahahah...)
![[Esercizio20210316.png]]

1. $\alpha=1$
2. $\alpha=0.1$
3. $\alpha=0.01$

!result

1. C
2. A
3. B

result!
(ce l'ho fatta...)
eg!

(ora c'è na tabella enorme...)

| Film | GMF | MM    | DD  | AB  |
| ---- | --- | ----- | --- | --- |
| F1   | 5   | 5     | 0   | 0   |
| F2   | 5   | ?=4.5 | ?=0 | 0   |
| F3   | ?=5 | 4     | 0   | ?=0 |
| F4   | 0   | 0     | 5   | 4   |
| F5   | 0   | 0     | 5   | ?=4 |

Features:
Romantico: F1,F2,F3
Azione: F4,F5

(In base alle features si può cercare di indovinare che valori avrebbero i ***?***)

Se formalizzassimo un algoritmo di raccomandazione avremmo due nuove colonne che identificherebbero le features ed un'ulteriore per il bias:

| Film | GMF($\theta^{(GMF)}$) | MM($\theta^{(MM)}$) | DD($\theta^{(DD)}$) | AB($\theta^{(AB)}$) | Romantico($x_1$) | Azione($x_2$) | Bias($x_0$) |
| ---- | --------------------- | ------------------- | ------------------- | ------------------- | ---------------- | ------------- | ----------- |
| F1   | 5                     | 5                   | 0                   | 0                   | 0.9              | 0             | 1           |
| F2   | 5                     | ?                   | ?                   | 0                   | 0.8              | 0.1           | 1           |
| F3   | ?                     | 4                   | 0                   | ?                   | 0.99             | 0             | 1           |
| F4   | 0                     | 0                   | 5                   | 4                   | 0                | 0.7           | 1           |
| F5   | 0                     | 0                   | 5                   | ?                   | 0.1              | 0.9           | 1           |

(mi sto confondendooooo... ok ha risolto le mie confusioni)
$h_\theta(\theta^{(GMF)}) = \theta^{(GMF)}_0+\theta^{(GMF)}_1x_1+\theta^{(GMF)}_2x_2$
e.g.
$\theta^{(GMF)}_0+\theta^{(GMF)}_10.9+\theta^{(GMF)}_20=5$

(si confonde anche il prof!)
$X^{(3)}=[x^{(3)}_0,x^{(3)}_1,x^{(3)}_2] = [1,0.9,0]$($x^{(i)}$ dove $i$ è la riga)
$y^{(3)}_2 = 4$($y^{(i)}_j$ dove $i$ è la riga e $j$ è la colonna)

